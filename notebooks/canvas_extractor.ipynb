{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Canvas access token from .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "token = os.environ.get(\"TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVIDER = 'swinburne'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "from datetime import datetime\n",
    "from tzlocal import get_localzone\n",
    "\n",
    "def parse_iso_timestamps(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all string timestamps in a DataFrame with datetime objects.\n",
    "    All timestamps must comply with ISO-8601.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame containing ISO-8601 string timestamps.\n",
    "\n",
    "    Returns:\n",
    "        data (DataFrame): DataFrame containing datetime timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain all columns which can be converted\n",
    "    # into a timestamp through ugly brute-forcing\n",
    "    timestamp_columns = []\n",
    "    for key, value in data.iloc[0].to_dict().items():\n",
    "        if type(value) is str:\n",
    "            try:\n",
    "                datetime.fromisoformat(value)\n",
    "                timestamp_columns.append(key)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def isotime_to_timestamp(value : str | None, use_local_timezone : bool = True, as_string : bool = False):\n",
    "        if type(value) is not str: return None\n",
    "        \n",
    "        time = datetime.fromisoformat(value)\n",
    "        if use_local_timezone:\n",
    "            time = time.astimezone(get_localzone())\n",
    "\n",
    "        if as_string:\n",
    "            time = time.strftime(\"%A, %d %B %Y, %I:%M %p\")\n",
    "            \n",
    "        return time\n",
    "    \n",
    "    for column in timestamp_columns:\n",
    "        data[column] = data[column].map(isotime_to_timestamp)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_canvas_dataframe(response : pd.DataFrame) -> pd.DataFrame:\n",
    "    response = parse_iso_timestamps(response)\n",
    "    return response\n",
    "    \n",
    "def decode_canvas_response(response : requests.Response) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts raw Canvas API response into a DataFrame for downstream use,\n",
    "    or raises Exception if the response is invalid.\n",
    "\n",
    "    Args:\n",
    "        response (Response): The raw API response obtained from requests.\n",
    "\n",
    "    Returns:\n",
    "        response_data (DataFrame): The response data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if response.ok:\n",
    "    \n",
    "        # Canvas API will always respond in JSON format,\n",
    "        # but we obtain the response in bytes, so it\n",
    "        # must be decoded into a raw string and then\n",
    "        # encoded into a JSON object.\n",
    "        \n",
    "        response_data = response.content.decode('utf-8')\n",
    "        \n",
    "        response_data = json.loads(response_data)\n",
    "\n",
    "        if not response_data:\n",
    "            # Empty response\n",
    "            return pd.DataFrame([])\n",
    "        \n",
    "        if type(response_data) is not list: response_data = [response_data]\n",
    "        \n",
    "        response_data = pd.DataFrame(response_data)\n",
    "\n",
    "        response_data = process_canvas_dataframe(response_data)\n",
    "    \n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get current units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two enrolment states:\n",
    "# active   - Visible on home menu\n",
    "# complete - Invisible\n",
    "\n",
    "units = requests.get(f\"https://swinburne.instructure.com/api/v1/courses/\", params={\n",
    "    \"access_token\":token,\n",
    "    \"enrollment_state\":\"complete\",\n",
    "    \"per_page\":100,\n",
    "})\n",
    "units = decode_canvas_response(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internally, Swinburne Organisation (ORG) units are assigned to enrollment term ID 1\n",
    "# Since we're only concerned with academic units, let's filter out organisation units\n",
    "units = units.loc[units.enrollment_term_id != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get assignments for current units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket (Status)\n",
    "# # UPCOMING:  unsubmitted\n",
    "# # OVERDUE:   overdue\n",
    "# # COMPLETE:  past\n",
    "\n",
    "# assignments = []\n",
    "\n",
    "# for unit_id in units[\"id\"].to_list():\n",
    "    \n",
    "#     response = requests.get(f\"https://swinburne.instructure.com/api/v1/courses/{unit_id}/assignments\", params={\n",
    "#         \"access_token\":token,\n",
    "#         \"order_by\":\"due_at\",\n",
    "#         \"per_page\":100, # Max items per unit\n",
    "#         \"bucket\":\"unsubmitted\",\n",
    "#     })\n",
    "    \n",
    "#     response = decode_canvas_response(response)\n",
    "#     assignments.append(response)\n",
    "\n",
    "# assignments = pd.concat(assignments).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(f\"https://swinburne.instructure.com/api/v1/courses/56983/assignments\", params={\n",
    "#     \"access_token\":token,\n",
    "#     \"per_page\":100, # Max items\n",
    "#     \"order_by\":\"due_at\" # Prioritise urgency\n",
    "# })\n",
    "# assignments = decode_canvas_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get assignment groups for current units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_groups = []\n",
    "\n",
    "for unit_id in units[\"id\"].to_list():\n",
    "    \n",
    "    response = requests.get(f\"https://swinburne.instructure.com/api/v1/courses/{unit_id}/assignment_groups\", params={\n",
    "        \"access_token\":token,\n",
    "        \"per_page\":100, # Max items per unit\n",
    "        \"include\":\"assignments\",\n",
    "    })\n",
    "    \n",
    "    response = decode_canvas_response(response)\n",
    "    assignment_groups.append(response)\n",
    "\n",
    "assignment_groups = pd.concat(assignment_groups).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract assignments from groups\n",
    "assignments = pd.json_normalize(assignment_groups.assignments.explode())\n",
    "assignments = assignments.dropna(subset=\"id\").reset_index(drop=True)\n",
    "assignments.id = assignments.id.astype(\"int\")\n",
    "assignments = process_canvas_dataframe(assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate grade contribution for each assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total amount of points possible for each assignment group\n",
    "# and add it as a feature for the data\n",
    "\n",
    "import numpy as np\n",
    "split = assignment_groups.explode(\"assignments\")\n",
    "\n",
    "entries = pd.json_normalize(split[\"assignments\"])\n",
    "entries = entries.add_prefix(\"assignment_\")\n",
    "\n",
    "split[\"assignment_id\"] = split.assignments.map(lambda x: x['id'] if type(x) is dict else None)\n",
    "split = split.dropna(subset=\"assignment_id\")\n",
    "split[\"assignment_id\"] = split[\"assignment_id\"].astype(\"int\")\n",
    "\n",
    "split = split.merge(entries, how=\"inner\", left_on=\"assignment_id\", right_on=\"assignment_id\")\n",
    "del split[\"assignments\"]\n",
    "\n",
    "total_points_possible = split.groupby(\"id\")[\"assignment_points_possible\"].sum().reset_index()\n",
    "total_points_possible = total_points_possible.rename(columns={\"assignment_points_possible\" : \"total_points_possible\"})\n",
    "assignment_groups = assignment_groups.merge(total_points_possible, how=\"outer\", left_on=\"id\", right_on=\"id\")\n",
    "split = split.merge(total_points_possible, how=\"outer\", left_on=\"id\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment_group_feature(assignment_id, group_feature_name : str, assignment_groups_split : pd.DataFrame = split):\n",
    "    try:\n",
    "        match = assignment_groups_split.loc[assignment_groups_split.assignment_id == assignment_id]\n",
    "        feature = match[group_feature_name].item()\n",
    "        return feature\n",
    "    except: return None\n",
    "\n",
    "assignments[\"points_total_for_group\"] = assignments[\"id\"].map(lambda x: get_assignment_group_feature(x, \"total_points_possible\"))\n",
    "assignments[\"group_weight\"] = assignments[\"id\"].map(lambda x: get_assignment_group_feature(x, \"group_weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments[\"score\"] = (assignments[\"points_possible\"] / assignments[\"points_total_for_group\"]) * (assignments[\"group_weight\"] / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments[[\"name\", \"course_id\", \"score\"]].sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get course name for each assignment from course ID\n",
    "\n",
    "This would be so much more efficient in SQL. It's not even a contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_name(course_id, units = units):\n",
    "    if course_id not in units.id.unique(): return None\n",
    "\n",
    "    name = units.loc[units.id == course_id].name.item()\n",
    "\n",
    "    return name\n",
    "\n",
    "assignments[\"course_name\"] = assignments.course_id.map( get_course_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments[[\"id\", \"course_name\", \"name\", \"due_at\", \"points_possible\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
